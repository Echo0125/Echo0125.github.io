(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{208:function(e,t,n){"use strict";var r=n(72);n.n(r).a},220:function(e,t,n){"use strict";n.r(t);n(208);var r=n(0),o=Object(r.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),n("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),n("p",[e._v("I am an M.S. student in Department of Computer Science and Technology, Nanjing University (NJU) since 2021, supervised by "),n("a",{attrs:{href:"https://cs.nju.edu.cn/lutong/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Tong Lu"),n("OutboundLink")],1),e._v(".\nI received my bachelor degree from Dalian Maritime University in 2021.")]),e._v(" "),n("p",[e._v("My research interests are computer vision and deep learning. I did some works about video understanding, online action detection, and action anticipation.")]),e._v(" "),n("h2",{attrs:{id:"news"}},[e._v("News")]),e._v(" "),n("ul",[n("li",[e._v("[2023-07-14] "),n("a",{attrs:{href:"https://arxiv.org/pdf/xxxx.yyyyy.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("MAT"),n("OutboundLink")],1),e._v(" is accepted by ICCV 2023.")]),e._v(" "),n("li",[e._v("[2023-05-22] We present a novel Video Sequence Understanding Framework "),n("a",{attrs:{href:"https://arxiv.org/abs/2305.13292",target:"_blank",rel:"noopener noreferrer"}},[e._v("VideoLLM"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2022-09-19] Our team wins "),n("strong",[e._v("Top-1")]),e._v(" rankings in "),n("strong",[e._v("7 tracks")]),e._v(" of "),n("a",{attrs:{href:"https://ego4d-data.org/workshops/eccv22/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ego4D ECCV2022 Challenge"),n("OutboundLink")],1),e._v(".")])]),e._v(" "),n("h2",{attrs:{id:"education-experiences"}},[e._v("Education & Experiences")]),e._v(" "),n("ul",[n("li",[n("p",[n("strong",[e._v("Nanjing University, Nanjing, China")]),e._v(" "),n("br"),e._v("\nSept 2021 - Present")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Dalian Maritime University, Liaoning, China")]),e._v(" "),n("br"),e._v("\nSept 2017 - June 2021")])])]),e._v(" "),n("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),n("p",[n("router-link",{attrs:{to:"/projects/"}},[e._v("â†’ Full list")])],1),e._v(" "),n("h2",{attrs:{id:"preprint"}},[e._v("Preprint")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/fast.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation")])]),e._v(" "),n("p",[e._v("Zhe Chen, "),n("strong",[e._v("Jiahao Wang")]),e._v(", Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu#\nArxiv, 2021")]),e._v(" "),n("p",[e._v("Introduction: We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector).")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2111.02394",target:"_blank",rel:"noopener noreferrer"}},[e._v("PDF"),n("OutboundLink")],1),e._v("]["),n("a",{attrs:{href:"https://czczup.github.io/bibtex/fast.txt",target:"_blank",rel:"noopener noreferrer"}},[e._v("bibtex"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/czczup/FAST",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("h2",{attrs:{id:"projects"}},[e._v("Projects")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/ego4d.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges")])]),e._v(" "),n("p",[e._v("Guo Chen*, Sen Xing*, Zhe Chen*, Yi Wang*, Kunchang Li, Yizhuo Li, Yi Liu, "),n("strong",[e._v("Jiahao Wang")]),e._v(", Yin-Dong Zheng, Bingkun Huang, Zhiyu Zhao, Junting Pan, Yifei Huang, Zun Wang, Jiashuo Yu, Yinan He, Hongjie Zhang, Tong Lu, Yali Wang, Limin Wang, Yu Qiao#\nArxiv, 2022")]),e._v(" "),n("p",[e._v("Introduction: This work presents our champion solutions to five tracks at Ego4D challenge.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2211.09529.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("PDF"),n("OutboundLink")],1),e._v("]["),n("a",{attrs:{href:"https://chenguo.netlify.app/bibtex/ego4d.txt",target:"_blank",rel:"noopener noreferrer"}},[e._v("bibtex"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/OpenGVLab/ego4d-eccv2022-solutions",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("h2",{attrs:{id:"awards-honors"}},[e._v("Awards & Honors")]),e._v(" "),n("h3",{attrs:{id:"contests"}},[e._v("Contests")]),e._v(" "),n("ul",[n("li",[e._v("[2022-10] 2nd Ego4D Challenge, ECCV2022,"),n("strong",[e._v("7 Top-1 Rankings")])])])],1)}),[],!1,null,null,null);t.default=o.exports},72:function(e,t,n){}}]);